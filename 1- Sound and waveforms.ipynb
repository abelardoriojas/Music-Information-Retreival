{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f52c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261.6255653005986\n"
     ]
    }
   ],
   "source": [
    "#Mapping pitch to frequency\n",
    "#midi notes are as follows:\n",
    "#A0 = 21, A1 = 33\n",
    "#C8 = 108, A4 = 69\n",
    "#From 21 to 108, in intervals of 12\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#12 Notes in an octave\n",
    "#Starts at C\n",
    "#C, C#, D, D#, F, F#, E, G, G#, A, A#, B\n",
    "\n",
    "def pitch2freq(p):\n",
    "    return (2**((p-69)/12))*440 #440 is standard A4, and is midi number 69\n",
    "\n",
    "#Frequency of middle C, C4 or midi number 60, is ~261 Hz\n",
    "\n",
    "print(pitch2freq(60))\n",
    "\n",
    "#Octaves are also divided into cents, where the pitch between semitones is divdided by 100\n",
    "\n",
    "#1200 cents in an octave, a noticeable difference in pitch is about 10-25 cents\n",
    "\n",
    "#Humans percieve sounds at very low intensities, the threshold of hearing is about 10^-12 W/m^2\n",
    "toh = 10e-12\n",
    "#Threshold of pain is 10 W/m^2\n",
    "\n",
    "#Since the human perception of sound covers 13 orders of magnitude, we use a log scale for intensity (decibels)\n",
    "from math import log10\n",
    "def intensity2dB(I):\n",
    "    return 10 * log10(I/toh)\n",
    "\n",
    "#If I is = toh, then we return 0 decibles\n",
    "#Everytime we go up by ~3db, intensity doubles\n",
    "\n",
    "#If frequency is the objective measure of pitch (subjective)\n",
    "#Then intensity is the objective measure of loudness (also subjective)\n",
    "\n",
    "#Humans percieve lower frequency sounds as quieter than higher freqency sounds, even if they have the same intensity.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a115dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-11\n"
     ]
    }
   ],
   "source": [
    "#TIMBRE\n",
    "\n",
    "#Timbre is the difference between two sounds that have the same intensity, frequency, and duration\n",
    "\n",
    "#For instance middle C played on a trumpet vs a piano\n",
    "\n",
    "#Timbre is multidimensional, meaning it has multiple features that all contribute to it\n",
    "#Sound envelope, harmonic content, and amplitude/frequency modulation\n",
    "\n",
    "#Sound evelope refers to the ADSR of the note\n",
    "\n",
    "#Attack (The amount of time it takes for the note to start and reach its highest ampliutude)\n",
    "#Decay (The amount of time it takes for the note to reach the highest amp to the sustain level)\n",
    "#Sustain (The amount of time the note remains at the same volume)\n",
    "#Release (The amount of time it takes for the note to fade)\n",
    "\n",
    "\n",
    "\n",
    "#The harmonic content refers to how intensity is split up among the different partial frequencies of a note, this gives the note it's color\n",
    "#The fundamental freq of the note has the highest intensity, with multiples of it layered over it\n",
    "#The distribution of intensity over those multiples is the harmonic content\n",
    "\n",
    "\n",
    "#Pitch and amp modulation\n",
    "\n",
    "#Notes can also change in frequency and intensity over time\n",
    "#This too contributes to the timbre of the sound\n",
    "\n",
    "#Notes that change in pitch have vibrato\n",
    "#Notes that change in amplitude have tremelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADC and DAC\n",
    "\n",
    "#ADC stands for analog to digital conversion\n",
    "\n",
    "#Since sound is composed of mechanical waves, the amplitude and time scales are continuous\n",
    "\n",
    "#You would need an infinite amount of memory to digitally store a sound\n",
    "\n",
    "#Have to convert to discrete values\n",
    "\n",
    "#We use sampling and quantization to do that\n",
    "\n",
    "#Sampling:\n",
    "    #Pretty self explanatory, the sound is sampled at various points on the time scale\n",
    "    #Sampling rate refers to how often the amplitude is recorded on the timescale\n",
    "    #The nyquist frequency is half of the sampling rate\n",
    "    #If there are frequencies above the nyquist frequency, then they will be aliased (moved to a lower frequency) when sampled\n",
    "    #For that reason, most sampling rates are around 40000 Hz, since the human hearing range caps out at 20k, all frequencies are below the nyquist frequency and therefore not aliased\n",
    "    \n",
    "\n",
    "#Quantization\n",
    "    #Pretty similar to sampling, except for the amplitude scale instead of the time scale\n",
    "    #Amplitude is 'sampled' onto an axis where each 'tick' or space is a binary value\n",
    "    #Resolution = number of bits\n",
    "    #CD resolution is 16 bits, meaning ampltiude is tracked over (2^16) 65536 possible values.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2bfd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Audio feature categorization\n",
    "\n",
    "#Level of abstraction\n",
    "    #Low level features: amplitude envelope, energy, spectral centriod, spectral flux, zero-crossing rate\n",
    "    #Mid level features: pitch and beat related descriptors, fluctuation patterns, MFCCs, note onsets (when they are struck)\n",
    "    #High level features: instrumentation, key, chords, melody, rhythm, tempo, lyrics, genre, mood\n",
    "#Temporal scope\n",
    "    #Instantenous (~50ms)\n",
    "    #Segment-level (seconds)\n",
    "    #Global (entire song)\n",
    "#Signal domain\n",
    "    #Some features are in the time domain only: amp env, zero-crossing rate, and RMSE. They are extracted from the raw waveform\n",
    "    #Others live in the frequency domain: Band energy ratio, spectral centriod, spectral flux (usually use Fourier Transform to analyse these)\n",
    "    #The final features are time-freq domain: spectrogram, MFCCs, and constant-q transform\n",
    "#ML approach\n",
    "    #Traditional ML techniques: SVMs, logistic and linear regression\n",
    "        #With these technqiues you hand pick certain audio features you feel would be important to solving some sort of audio classification task, and feed into your svm, or regression\n",
    "    #Deep learning techniques:\n",
    "        #With DL technqiues, you usually send the whole raw audio, and a spectrogram. And then the features are extracted from there automatically and fed into the algorithm\n",
    "        \n",
    "#Audio feature pipeline\n",
    "    #Time domain features:\n",
    "        #Natural audio is fed through an ADC, sampled and quantized, each sample is (1/44100) =~ .027 ms\n",
    "        #Samples are 'framed' or put into percieveable chunks of audio\n",
    "        #Ear resolution is 10ms\n",
    "        #Frames are always of size 2^x in sample length, because having frames that are log2 speeds up the fast fourier transform\n",
    "        #Typical values 256-8192 samples\n",
    "        #Time domain features are then calculated on the frames\n",
    "        #And then aggregated together (mean, median, gaussian mixture models)\n",
    "        #Then we have a final feature value, vector, or matrix\n",
    "    #Frequency domain features:\n",
    "        #Natural audio is fed through an ADC, and framed again\n",
    "        #The frames are then moved from the time domain to the frequency domain using the fourier transform\n",
    "        #BUT! We have to worry about spectral leakage\n",
    "        #When the frame contains parts of a signal that do not conver it's entire period, the endpoints are discontinuous\n",
    "        #This causes artifacts (higher frequencies) to show up in the frequency graphs\n",
    "        #For this reason frames are 'windowed' using a windowing function to preserve the mid-section of a sample\n",
    "        #While silencing the endpoints\n",
    "        #This is also why the frames overlap, so that the entire audio signal is still processed\n",
    "        #'hop' length refers to the amount of shift to the right in samples in the frame\n",
    "        #Once the singal is framed and windowed we pass it into the fourier transform\n",
    "        #And extract our low level frequency domain features, and aggreate into a feature value, vector, or matrix\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53638b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time domain features in depth\n",
    "\n",
    "#Ampltiude envelope\n",
    "    #The maximum amplitude value of all samples in a frame\n",
    "    #Gives rough idea of loudness\n",
    "    #Sensitive to outliers\n",
    "    #Very useful to onset detection, finding the point in the signal where a note is struck\n",
    "    #Some higher level abstractions as well like genre classification (some genres are louder than others, folk vs metal)\n",
    "#Root mean square energy\n",
    "    #This is the root mean square of all the samples in a frame\n",
    "    #Also an indicator of loudness\n",
    "    #Less sensitive to outliers\n",
    "    #Very useful for audio segmentation (consolidating up instantenous features into second-long segments) as RSME changes a lot when a new musical event happens\n",
    "#Zero crossing rate\n",
    "    #Number of times a signal crosses the horizontal axis\n",
    "    #Recongition of percussive vs pitched sounds\n",
    "    #Percussive noises tend to have random ZCRs while pitched noises tend to have much more stable ZCRs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
